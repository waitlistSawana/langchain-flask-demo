{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain quickstart\n",
    "\n",
    "- 对话bot和promt创建\n",
    "\n",
    "- 历史对话记忆模块\n",
    "\n",
    "- 相似度检索\n",
    "\n",
    "- 构建runnable的chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/use_cases/chatbots/quickstart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.24.0 requires tokenizers!=0.11.3,<0.14,>=0.11.1, but you have tokenizers 0.19.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-openai langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4-1106-preview\", \n",
    "    temperature=0.2,\n",
    "    base_url = os.environ.get('OPEN_AI_BASE_URL'),\n",
    "    api_key = os.environ.get('OPEN_AI_GPT4_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore programmer.\", response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 19, 'total_tokens': 25}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': 'fp_2f57f81c11', 'finish_reason': 'stop', 'logprobs': None}, id='run-63bc568e-1610-463c-a7b2-f6cbeefb8019-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI language model, I don't have a continuous conversation history, so I can't recall what I said previously. However, if you're referring to the last message I provided in our current interaction, it would be the response to your previous query or statement. If you need me to repeat or clarify anything, please let me know what specific information you're looking for, and I'll be happy to help!\", response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 13, 'total_tokens': 97}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': 'fp_2f57f81c11', 'finish_reason': 'stop', 'logprobs': None}, id='run-ff1c6f7b-0ff4-427c-8765-21dcc1b94d0f-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke([HumanMessage(content=\"What did you just say?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated the sentence \"I love programming\" into French, which is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 41, 'total_tokens': 64}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': 'fp_2f57f81c11', 'finish_reason': 'stop', 'logprobs': None}, id='run-9677f8be-6d8e-4c0d-910c-58ba1466014b-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"J'adore la programmation.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated the sentence \"I love programming\" into French, which is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 61, 'total_tokens': 84}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': 'fp_2f57f81c11', 'finish_reason': 'stop', 'logprobs': None}, id='run-928eca9b-0288-43d8-a1a9-479fe751cec9-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate this sentence from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!'), AIMessage(content='whats up?')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory() # 创建消息管理实例\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"hi!\") # 加入用户消息\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(\"whats up?\") # 加入ai消息\n",
    "\n",
    "demo_ephemeral_chat_history.messages # 查看消息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime la programmation.\", response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 53, 'total_tokens': 60}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': 'fp_2f57f81c11', 'finish_reason': 'stop', 'logprobs': None}, id='run-318c4b2e-cc95-49dd-9cb9-33950bdd2779-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_user_message(\n",
    "    \"Translate this sentence from English to French: I love programming.\"\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated \"I love programming\" into French, which is \"J\\'aime la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 74, 'total_tokens': 94}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': 'fp_2f57f81c11', 'finish_reason': 'stop', 'logprobs': None}, id='run-4d92e621-a4b5-43a1-892e-67ecc064e5be-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"What did you just say?\")\n",
    "\n",
    "chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-chroma beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) # 每块有500字 0重叠区域\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存为向量数据库\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits, # 要保存的切分好的内容 Document文件\n",
    "    embedding=OpenAIEmbeddings(\n",
    "        base_url = os.environ.get('OPEN_AI_BASE_URL'),\n",
    "        api_key = os.environ.get('OPEN_AI_GPT4_API_KEY')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       " Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring & AutomationsPrompt HubProxyPricingSelf-HostingCookbookQuick StartOn this pageGetting started with LangSmithIntroductionâ€‹LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!Install', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       " Document(page_content='reach out to us at support@langchain.dev.My team deals with sensitive data that cannot be logged. How can I ensure that only my team can access it?â€‹If you are interested in a private deployment of LangSmith or if you need to self-host, please reach out to us at sales@langchain.dev. Self-hosting LangSmith requires an annual enterprise license that also comes with support and formalized access to the LangChain team.Was this page helpful?NextUser GuideIntroductionInstall LangSmithCreate an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       " Document(page_content='lifecycle.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resourcesâ€‹LangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k is the number of chunks to retrieve\n",
    "retriever = vectorstore.as_retriever(k=4) \n",
    "\n",
    "docs = retriever.invoke(\"how can langsmith help with testing?\")\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handling documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "cchat = ChatOpenAI(\n",
    "    model=\"gpt-4-1106-preview\", \n",
    "    temperature=0.2,\n",
    "    base_url = os.environ.get('OPEN_AI_BASE_URL'),\n",
    "    api_key = os.environ.get('OPEN_AI_GPT4_API_KEY')\n",
    ")\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangSmith can help with testing your LLM (Large Language Model) applications in several ways:\\n\\n1. **Evaluation**: LangSmith provides evaluation capabilities that allow you to assess the performance of your LLM applications. You can test the model's responses to various prompts to ensure that it is generating the expected outputs.\\n\\n2. **Tracing**: With tracing features, you can track the requests and responses in your application. This is useful for debugging and understanding how your LLM is performing in different scenarios. It can help you identify patterns or issues that may not be apparent during initial development.\\n\\n3. **Production Monitoring & Automations**: Once your application is live, LangSmith offers monitoring tools to ensure that it continues to perform as expected. You can set up automations to handle specific events or conditions, which can be critical for maintaining the quality and reliability of your service.\\n\\n4. **Prompt Hub**: The Prompt Hub is a management tool that helps you organize and manage the prompts you use for testing. It can be a valuable resource for creating a structured testing framework, allowing you to easily reuse and iterate on prompts.\\n\\nBy leveraging these features, LangSmith can help you thoroughly test your LLM applications, ensuring they are robust, reliable, and ready for production use.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"how can langsmith help with testing?\")\n",
    "\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "        \"context\": docs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建检索链"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“链chain”是langchain的核心概念\n",
    "\n",
    "它包含着`输入-处理单元-输出-作为另一个单元的输入---`的概念\n",
    "\n",
    "输入通常通过`.invoke`关键字传入，输出结果就是invoke之后的对象\n",
    "\n",
    "而链条通过`|`符号来构建，它代表了前者的输出会作为输入传递给下一个内容，它可以被看做是一个函数\n",
    "\n",
    "同时，处理单元不一定是lanchain的默认组件，也可以自己定义，它是一个函数，包含输入`(params)`和输出`return ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/expression_language/primitives/assign/\n",
    "\n",
    "`RunnableParallel`和`RunnablePassthrough`是用来构建chain的两个常用方法\n",
    "\n",
    "`RunnableParallel`创建的链会返回以函数名命名key的字典，并且其输出value是结果值 `{'modified': 2}`\n",
    "\n",
    "`RunnablePassthrough`创建的链与上一个方法的区别在于他的value不只是值，而是包括输入输出的字典 `{'extra': {'num': 1, 'mult': 3}}`\n",
    "\n",
    "它也经常和`.assign`一起使用，用来定义输出值的key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "\n",
    "\n",
    "def parse_retriever_input(params: Dict):\n",
    "    return params[\"messages\"][-1].content # 构建一个处理单元\n",
    "\n",
    "\n",
    "retrieval_chain = RunnablePassthrough.assign( # https://python.langchain.com/docs/expression_language/primitives/assign/\n",
    "    context=parse_retriever_input | retriever,\n",
    ").assign(\n",
    "    answer=document_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='how can langsmith help with testing?')],\n",
       " 'context': [Document(page_content='Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring & AutomationsPrompt HubProxyPricingSelf-HostingCookbookQuick StartOn this pageGetting started with LangSmithIntroductionâ€‹LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!Install', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='reach out to us at support@langchain.dev.My team deals with sensitive data that cannot be logged. How can I ensure that only my team can access it?â€‹If you are interested in a private deployment of LangSmith or if you need to self-host, please reach out to us at sales@langchain.dev. Self-hosting LangSmith requires an annual enterprise license that also comes with support and formalized access to the LangChain team.Was this page helpful?NextUser GuideIntroductionInstall LangSmithCreate an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='lifecycle.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resourcesâ€‹LangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'})],\n",
       " 'answer': \"LangSmith can help with testing your LLM (Large Language Model) applications in several ways:\\n\\n1. **Evaluation**: LangSmith provides evaluation capabilities that allow you to assess the performance of your LLM applications. This can include testing for accuracy, relevance, and coherence of the responses generated by your model.\\n\\n2. **Tracing**: With tracing capabilities, you can track the behavior of your LLM applications. This feature enables you to understand how your application processes inputs and generates outputs, which is crucial for debugging and improving the model's performance.\\n\\n3. **Production Monitoring & Automations**: LangSmith offers tools for monitoring your LLM applications in production. This includes the ability to set up automated alerts and actions based on specific criteria or performance metrics, helping you to quickly identify and address issues.\\n\\n4. **Prompt Hub**: The Prompt Hub is a management tool that allows you to organize and manage prompts used in your LLM applications. By testing different prompts and tracking their performance, you can optimize the interactions with your model for better results.\\n\\n5. **Cookbook**: The LangSmith Cookbook provides a collection of tutorials and end-to-end walkthroughs that can be used as a guide for testing and implementing various features of your LLM applications.\\n\\nBy utilizing these features, LangSmith helps ensure that your LLM applications are robust, reliable, and ready for production use.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "    }\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='how can langsmith help with testing?'),\n",
       "  AIMessage(content=\"LangSmith can help with testing your LLM (Large Language Model) applications in several ways:\\n\\n1. **Evaluation**: LangSmith provides evaluation capabilities that allow you to assess the performance of your LLM applications. This can include testing for accuracy, relevance, and coherence of the responses generated by your model.\\n\\n2. **Tracing**: With tracing capabilities, you can track the behavior of your LLM applications. This feature enables you to understand how your application processes inputs and generates outputs, which is crucial for debugging and improving the model's performance.\\n\\n3. **Production Monitoring & Automations**: LangSmith offers tools for monitoring your LLM applications in production. This includes the ability to set up automated alerts and actions based on specific criteria or performance metrics, helping you to quickly identify and address issues.\\n\\n4. **Prompt Hub**: The Prompt Hub is a management tool that allows you to organize and manage prompts used in your LLM applications. By testing different prompts and tracking their performance, you can optimize the interactions with your model for better results.\\n\\n5. **Cookbook**: The LangSmith Cookbook provides a collection of tutorials and end-to-end walkthroughs that can be used as a guide for testing and implementing various features of your LLM applications.\\n\\nBy utilizing these features, LangSmith helps ensure that your LLM applications are robust, reliable, and ready for production use.\"),\n",
       "  HumanMessage(content='tell me more about that!')],\n",
       " 'context': [Document(page_content=\"keySetup your environmentLog your first traceCreate your first evaluationNext StepsAdditional ResourcesFAQHow do I migrate projects between organizations?Why aren't my runs aren't showing up in my project?My team deals with sensitive data that cannot be logged. How can I ensure that only my team can access it?CommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.\", metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='lifecycle.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resourcesâ€‹LangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='reach out to us at support@langchain.dev.My team deals with sensitive data that cannot be logged. How can I ensure that only my team can access it?â€‹If you are interested in a private deployment of LangSmith or if you need to self-host, please reach out to us at sales@langchain.dev. Self-hosting LangSmith requires an annual enterprise license that also comes with support and formalized access to the LangChain team.Was this page helpful?NextUser GuideIntroductionInstall LangSmithCreate an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'})],\n",
       " 'answer': \"Certainly! Let's delve deeper into how LangSmith can assist with testing your LLM applications:\\n\\n1. **Evaluation Capabilities**: LangSmith's evaluation tools allow you to measure the performance of your LLM against specific benchmarks or datasets. You can create evaluations to test the model's understanding, ability to generate correct and relevant responses, and adherence to certain guidelines or criteria. This can help you fine-tune your model before deploying it to production.\\n\\n2. **Tracing Capabilities**: Tracing is a powerful feature for understanding the decision-making process of your LLM. By logging the intermediate steps the model takes when processing inputs, you can gain insights into how it arrives at its conclusions. This is particularly useful for identifying why a model might be making errors, which parts of the input it's focusing on, and how different components of your system interact with each other.\\n\\n3. **Production Monitoring & Automations**: Once your LLM application is live, it's important to continuously monitor its performance to ensure it's operating as expected. LangSmith can help you set up monitoring systems that track key metrics, such as response time, accuracy, and user satisfaction. Automations can be configured to respond to certain events, like a drop in performance, by alerting your team or even triggering a rollback to a previous version of the model.\\n\\n4. **Prompt Hub**: The Prompt Hub within LangSmith is a centralized place to manage the prompts you use to communicate with your LLM. By systematically testing different prompts and tracking their effectiveness, you can optimize how you interact with the model. This can lead to more consistent and accurate responses from the LLM, improving the overall user experience.\\n\\n5. **LangSmith Cookbook**: The Cookbook is a resource that compiles practical examples, tutorials, and walkthroughs for using LangSmith. It can serve as a guide for setting up tests, integrating LangSmith features into your workflow, and learning best practices for working with LLMs. The Cookbook can accelerate your testing process by providing ready-to-use recipes for common tasks and challenges.\\n\\nBy leveraging these features, LangSmith can significantly enhance the testing and development process of your LLM applications, helping you to build more reliable and effective AI-powered solutions.\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_ai_message(response[\"answer\"])\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"tell me more about that!\")\n",
    "\n",
    "retrieval_chain.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Certainly! Let's delve deeper into how LangSmith can assist with testing your LLM applications:\\n\\n1. **Evaluation Capabilities**:\\n   - LangSmith's evaluation tools allow you to measure the performance of your LLM against specific benchmarks or datasets. This can help you understand how well your model is performing in terms of accuracy, fluency, and adherence to the task it's designed for.\\n   - You can create evaluations that mimic real-world scenarios to see how your LLM would perform in actual use cases.\\n   - The evaluation results can guide you in fine-tuning your model or making adjustments to improve its performance.\\n\\n2. **Tracing Capabilities**:\\n   - Tracing is a powerful feature for understanding the decision-making process of your LLM. It provides insights into the intermediate steps the model takes when generating responses.\\n   - By analyzing traces, you can identify patterns or errors in the model's processing, which can be invaluable for debugging and improving the model's logic.\\n   - Tracing can also help you ensure that the model's outputs are explainable and transparent, which is important for applications in regulated industries.\\n\\n3. **Production Monitoring & Automations**:\\n   - Monitoring tools within LangSmith can track the performance of your LLM in a live environment. This includes monitoring response times, error rates, and usage patterns.\\n   - You can set up automated alerts to notify your team when certain thresholds are breached, such as an increase in error rates or a drop in performance metrics.\\n   - Automations can also be configured to take corrective actions, such as rolling back to a previous version of the model if a new deployment causes issues.\\n\\n4. **Prompt Hub**:\\n   - The Prompt Hub allows you to store, categorize, and manage the prompts you use to interact with your LLM.\\n   - By systematically testing different prompts and variations, you can determine which ones yield the best results for your specific use cases.\\n   - The Prompt Hub can also facilitate A/B testing, enabling you to compare the performance of different prompts in a controlled manner.\\n\\n5. **LangSmith Cookbook**:\\n   - The Cookbook is a resource that contains practical guides and examples for implementing and testing various features with your LLM.\\n   - It can serve as a starting point for new users to get acquainted with best practices and advanced techniques in LLM application development.\\n   - The walkthroughs can help you avoid common pitfalls and accelerate the development and testing of your LLM applications.\\n\\nBy leveraging these features, LangSmith provides a comprehensive suite of tools that can support the entire lifecycle of LLM application development, from initial testing and evaluation to deployment and ongoing monitoring in production environments.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain_with_only_answer = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=parse_retriever_input | retriever,\n",
    "    )\n",
    "    | document_chain\n",
    ")\n",
    "\n",
    "retrieval_chain_with_only_answer.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查询转换\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# We need a prompt that we can pass into an LLM to generate a transformed search query\n",
    "\n",
    "cchat = ChatOpenAI(\n",
    "    model=\"gpt-4-1106-preview\", \n",
    "    temperature=0.2,\n",
    "    base_url = os.environ.get('OPEN_AI_BASE_URL'),\n",
    "    api_key = os.environ.get('OPEN_AI_GPT4_API_KEY')\n",
    ")\n",
    "\n",
    "query_transform_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_transforming_retriever_chain = RunnableBranch(\n",
    "    (\n",
    "        lambda x: len(x.get(\"messages\", [])) == 1,\n",
    "        # If only one message, then we just pass that message's content to retriever\n",
    "        (lambda x: x[\"messages\"][-1].content) | retriever,\n",
    "    ),\n",
    "    # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\n",
    "    query_transform_prompt | chat | StrOutputParser() | retriever,\n",
    ").with_config(run_name=\"chat_retriever_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)\n",
    "\n",
    "conversational_retrieval_chain = RunnablePassthrough.assign(\n",
    "    context=query_transforming_retriever_chain,\n",
    ").assign(\n",
    "    answer=document_chain,\n",
    ")\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='how can langsmith help with testing?'),\n",
       "  AIMessage(content=\"LangSmith can help with testing your LLM (Large Language Model) applications in several ways:\\n\\n1. **Evaluation**: LangSmith provides evaluation capabilities that allow you to assess the performance of your LLM applications. This can include testing the model's responses for accuracy, relevance, and coherence. You can set up various tests to ensure that the model is performing as expected under different scenarios.\\n\\n2. **Tracing**: With tracing features, you can track and analyze how your LLM applications are being used. This can help you identify any issues or areas for improvement by providing insights into the model's decision-making process. Tracing can be particularly useful for debugging and understanding the model's outputs.\\n\\n3. **Production Monitoring & Automations**: LangSmith offers tools for monitoring your LLM applications in production. This includes tracking metrics, setting up alerts, and automating responses to certain conditions. Monitoring can help you quickly identify and respond to problems that arise during testing or after deployment.\\n\\n4. **Prompt Hub**: The Prompt Hub is a prompt management tool that allows you to organize and manage the prompts you use for testing your LLM applications. This can help ensure consistency in testing and make it easier to iterate on prompts to achieve the desired outcomes.\\n\\nBy utilizing these features, LangSmith can help streamline the testing process, making it easier to develop, deploy, and maintain reliable and high-quality LLM applications.\")],\n",
       " 'context': [Document(page_content='Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring & AutomationsPrompt HubProxyPricingSelf-HostingCookbookQuick StartOn this pageGetting started with LangSmithIntroductionâ€‹LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!Install', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='reach out to us at support@langchain.dev.My team deals with sensitive data that cannot be logged. How can I ensure that only my team can access it?â€‹If you are interested in a private deployment of LangSmith or if you need to self-host, please reach out to us at sales@langchain.dev. Self-hosting LangSmith requires an annual enterprise license that also comes with support and formalized access to the LangChain team.Was this page helpful?NextUser GuideIntroductionInstall LangSmithCreate an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='lifecycle.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resourcesâ€‹LangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'})],\n",
       " 'answer': \"LangSmith can help with testing your LLM (Large Language Model) applications in several ways:\\n\\n1. **Evaluation**: LangSmith provides evaluation capabilities that allow you to assess the performance of your LLM applications. This can include testing the model's responses for accuracy, relevance, and coherence. You can set up various tests to ensure that the model is performing as expected under different scenarios.\\n\\n2. **Tracing**: With tracing features, you can track and analyze how your LLM applications are being used. This can help you identify any issues or areas for improvement by providing insights into the model's decision-making process. Tracing can be particularly useful for debugging and understanding the model's outputs.\\n\\n3. **Production Monitoring & Automations**: LangSmith offers tools for monitoring your LLM applications in production. This includes tracking metrics, setting up alerts, and automating responses to certain conditions. Monitoring can help you quickly identify and respond to problems that arise during testing or after deployment.\\n\\n4. **Prompt Hub**: The Prompt Hub is a prompt management tool that allows you to organize and manage the prompts you use for testing your LLM applications. This can help ensure consistency in testing and make it easier to iterate on prompts to achieve the desired outcomes.\\n\\nBy utilizing these features, LangSmith can help streamline the testing process, making it easier to develop, deploy, and maintain reliable and high-quality LLM applications.\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_user_message(\"how can langsmith help with testing?\")\n",
    "\n",
    "response = conversational_retrieval_chain.invoke(\n",
    "    {\"messages\": demo_ephemeral_chat_history.messages},\n",
    ")\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(response[\"answer\"])\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='how can langsmith help with testing?'),\n",
       "  AIMessage(content=\"LangSmith can help with testing your LLM (Large Language Model) applications in several ways:\\n\\n1. **Evaluation**: LangSmith provides evaluation capabilities that allow you to assess the performance of your LLM applications. This can include testing the model's responses for accuracy, relevance, and coherence. You can set up various tests to ensure that the model is performing as expected under different scenarios.\\n\\n2. **Tracing**: With tracing features, you can track and analyze how your LLM applications are being used. This can help you identify any issues or areas for improvement by providing insights into the model's decision-making process. Tracing can be particularly useful for debugging and understanding the model's outputs.\\n\\n3. **Production Monitoring & Automations**: LangSmith offers tools for monitoring your LLM applications in production. This includes tracking metrics, setting up alerts, and automating responses to certain conditions. Monitoring can help you quickly identify and respond to problems that arise during testing or after deployment.\\n\\n4. **Prompt Hub**: The Prompt Hub is a prompt management tool that allows you to organize and manage the prompts you use for testing your LLM applications. This can help ensure consistency in testing and make it easier to iterate on prompts to achieve the desired outcomes.\\n\\nBy utilizing these features, LangSmith can help streamline the testing process, making it easier to develop, deploy, and maintain reliable and high-quality LLM applications.\"),\n",
       "  HumanMessage(content='tell me more about that!')],\n",
       " 'context': [Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring & AutomationsPrompt HubProxyPricingSelf-HostingCookbookQuick StartOn this pageGetting started with LangSmithIntroductionâ€‹LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!Install', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='lifecycle.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resourcesâ€‹LangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'}),\n",
       "  Document(page_content='reach out to us at support@langchain.dev.My team deals with sensitive data that cannot be logged. How can I ensure that only my team can access it?â€‹If you are interested in a private deployment of LangSmith or if you need to self-host, please reach out to us at sales@langchain.dev. Self-hosting LangSmith requires an annual enterprise license that also comes with support and formalized access to the LangChain team.Was this page helpful?NextUser GuideIntroductionInstall LangSmithCreate an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith'})],\n",
       " 'answer': \"Certainly! Let's delve a bit deeper into how LangSmith can assist with testing your LLM applications:\\n\\n1. **Evaluation Tools**: \\n   - LangSmith's evaluation tools enable you to conduct comprehensive testing of your LLM's performance. You can create specific datasets with expected outputs to benchmark your model's responses.\\n   - You might also be able to perform A/B testing to compare different versions of your model or different configurations to determine which performs best.\\n   - The platform may provide analytics to help you understand the model's accuracy, precision, recall, and other relevant metrics that are crucial for evaluating AI models.\\n\\n2. **Tracing Features**: \\n   - Tracing allows you to record and visualize the sequence of events that occur when your LLM processes a request. This can include the input data, the model's intermediate steps, and the final output.\\n   - With tracing, you can pinpoint where errors or unexpected behaviors occur, which is essential for debugging and refining your model.\\n   - Tracing can also help you understand how your model is being used in real-world scenarios, which can inform further development and testing.\\n\\n3. **Production Monitoring & Automations**: \\n   - Once your LLM application is live, LangSmith's monitoring tools can help you keep an eye on its performance and health. You can track usage patterns, response times, and error rates.\\n   - Automations can be set up to handle routine tasks or to respond to certain conditions automatically. For example, if an error rate exceeds a certain threshold, an alert can be triggered, or a fallback mechanism can be activated.\\n   - This proactive monitoring can help you maintain high availability and performance standards for your LLM applications.\\n\\n4. **Prompt Hub**: \\n   - The Prompt Hub serves as a centralized repository for managing and versioning the prompts you use to interact with your LLM. This ensures that you can maintain consistency across tests and easily roll back to previous versions if needed.\\n   - It can also facilitate collaboration among team members who are working on prompt development and testing, as they can share and refine prompts within the hub.\\n   - By organizing prompts effectively, you can more efficiently test different aspects of your LLM's capabilities and ensure that it handles a wide range of inputs as expected.\\n\\nBy leveraging these features, LangSmith not only helps you test and evaluate your LLM applications but also ensures that you can maintain and improve them throughout their lifecycle. Whether you're in the early stages of development or managing a live application, LangSmith provides the tools necessary to ensure your LLM applications are robust, reliable, and ready for production use.\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_user_message(\"tell me more about that!\")\n",
    "\n",
    "conversational_retrieval_chain.invoke(\n",
    "    {\"messages\": demo_ephemeral_chat_history.messages}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
